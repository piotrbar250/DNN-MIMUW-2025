{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDib219Y1e4z"
      },
      "source": [
        "In this lab scenario you will finish an implementation of a variant of the Q-learning method called DQN.\n",
        "\n",
        "On top of the usual Q-learning, using neural nets as function approximations, DQN uses:\n",
        "* experience replay – used to increase efficacy of samples from the environment and decorrelate elements of a batch;\n",
        "* target network – used to avoid constantly changing targets in the learning process (to avoid \"chasing your own tail\").\n",
        "\n",
        "For the algorithm's details recall the lecture and/or follow the [original paper](https://arxiv.org/abs/1312.5602), which is rather self-contained and not hard to understand.\n",
        "\n",
        "Without changing any hyperparameters, the agent should solve the problem (obtain rewards \\~200) after \\~500 episodes (\\~13 minutes of training on Colab CPU, not any faster on GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFUv8EnMnh8"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "1.   Implement missing code #### TODO IMPLEMENT #####\n",
        "2.   Experiment with the hyperparameters e.g. gamma (discount-factor in accumulated reward), epsilon (exploration-exploitation trade-off in \"epsilon-greedy\")\n",
        "3.   Observe weird behaviors of agents, e.g. \"forgetting how to play\" (reward going down a lot and then \"re-learning\" again). Why can that happen? What can we do to avoid it?\n",
        "4.   Change the args and observe the trained model behavior. What do you see?\n",
        "5.   What can be improved in the training code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbWvFo-wAP_N"
      },
      "source": [
        "## Imports\n",
        "(Some extra packages are needed even on Colab. For some reason `box2d` needs to be installed in a separate step after `swig`, and will take a minute to build)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5B0s1pZtM8N3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: swig==4.4.* in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (4.4.1)\n",
            "Requirement already satisfied: gymnasium[box2d]==1.2.* in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (0.0.4)\n",
            "Requirement already satisfied: box2d==2.3.10 in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (2.3.10)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /home/piotrbaranowski/.venvs/jupyter2/lib/python3.11/site-packages (from gymnasium[box2d]==1.2.*) (4.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'swig==4.4.*'\n",
        "!pip install 'gymnasium[box2d]==1.2.*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Le1nHKNN9_xa"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "from abc import ABC\n",
        "from collections.abc import Sequence\n",
        "from copy import deepcopy\n",
        "from functools import reduce\n",
        "from pathlib import Path\n",
        "from typing import Literal, NamedTuple, cast\n",
        "\n",
        "import cv2\n",
        "import IPython.display as display\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "# Suppress a warning from pygame: https://github.com/pygame/pygame/issues/4313\n",
        "warnings.filterwarnings(\"ignore\", message=\"(?s:.)*pkg_resources is deprecated\")\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bdpxp-q6Mnh-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True cuda\n"
          ]
        }
      ],
      "source": [
        "# Check for CUDA / MPS (Apple) / XPU (Intel) / ... accelerator.\n",
        "device = (\n",
        "    torch.accelerator.current_accelerator(check_available=True)\n",
        "    or torch.device(\"cpu\")  #\n",
        ")\n",
        "use_accel = device != torch.device(\"cpu\")\n",
        "print(use_accel, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bOWge0rWMnh-"
      },
      "outputs": [],
      "source": [
        "# Used to speed up the training in Colab and avoid situations when the model is stuck\n",
        "# But this can be problematic for our model (why?)\n",
        "# Use -1 for unlimited.\n",
        "MAX_EPISODE_STEPS = 500\n",
        "\n",
        "EXP_NAME = \"LunarLander\"\n",
        "LOG_DIR = Path(\"./runs/\") / EXP_NAME\n",
        "TENSORBOARD_LOG_DIR = LOG_DIR / \"tensorboard\"\n",
        "CHECKPOINTS_DIR = LOG_DIR / \"checkpoints\"\n",
        "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TENSORBOARD_LOG_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOwnttyYMnh-"
      },
      "source": [
        "## Environment\n",
        "\n",
        "We will try to solve: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "LunearLander env can be considered solved once we achieve 200 points.\n",
        "\n",
        "![Lunar Lander example GIF](https://gymnasium.farama.org/_images/lunar_lander.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yNfqvem5Mnh-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OBS_SHAPE=(8,) N_ACTIONS=4\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\n",
        "    \"LunarLander-v3\", render_mode=\"rgb_array\", max_episode_steps=MAX_EPISODE_STEPS\n",
        ")\n",
        "\n",
        "# Observations are 8-dimensional vectors: (x, y, vx, vy, theta, vtheta, left_leg, right_leg),\n",
        "# where left_leg, right_leg are booleans: 1 if leg has contact, 0 otherwise.\n",
        "OBS_SHAPE = cast(tuple[int, ...], env.observation_space.shape)\n",
        "\n",
        "# Actions are: 0=do nothing, 1=fire left engine, 2=fire main engine, 3=fire right engine.\n",
        "assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "N_ACTIONS = int(env.action_space.n)\n",
        "\n",
        "print(f\"{OBS_SHAPE=} {N_ACTIONS=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8,)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# OBS_SHAPE\n",
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.5796, 0.9784, 0.8845, 0.3057, 0.0127, 0.9888, 0.3784, 0.4501])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.rand((8,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGKDXkY-U9C"
      },
      "source": [
        "## Scheduler\n",
        "(for the exploration rate $\\epsilon$ in \"$\\epsilon$-greedy\")\n",
        "\n",
        "Training RL agents requires dealing with exploration-exploitation trade-off. To handle this we will adopt the most basic, but extremely efficient, epsilon-greedy strategy. At the beginning our agent will focus on exploration, and over time will start exploiting his knowledge, and thus becoming more and more greedy. To implement this logic we will use LinearDecay or ExponentialRateScheduler scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zl5GIWFJ-bsp"
      },
      "outputs": [],
      "source": [
        "class ExplorationRateScheduler(ABC):\n",
        "    def __init__(self, initial: float, on: Literal[\"step\", \"episode\"]) -> None:\n",
        "        self._value = initial\n",
        "        self.on = on\n",
        "\n",
        "    def value(self) -> float:\n",
        "        \"\"\"Returns the cur rent exploration rate (epsilon) without updating it.\"\"\"\n",
        "        return self._value\n",
        "\n",
        "    def _update(self) -> None:\n",
        "        \"\"\"Updates the exploration rate (epsilon).\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def next_step(self) -> float:\n",
        "        \"\"\"Updates the exploration rate (epsilon) and returns the new value.\"\"\"\n",
        "        if self.on == \"step\":\n",
        "            self._update()\n",
        "        return self.value()\n",
        "\n",
        "    def next_episode(self) -> float:\n",
        "        \"\"\"Updates the exploration rate (epsilon) and returns the new value.\"\"\"\n",
        "        if self.on == \"episode\":\n",
        "            self._update()\n",
        "        return self.value()\n",
        "\n",
        "\n",
        "class ConstantRateScheduler(ExplorationRateScheduler):\n",
        "    \"\"\"Constant scheduler (use epsilon=0 for greedy policy).\"\"\"\n",
        "\n",
        "    def __init__(self, epsilon: float) -> None:\n",
        "        super().__init__(initial=epsilon, on=\"step\")\n",
        "\n",
        "\n",
        "class LinearRateScheduler(ExplorationRateScheduler):\n",
        "    \"\"\"Linear scheduler (step() decreases value by `rate`, until final is reached).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        initial: float,\n",
        "        final: float,\n",
        "        rate: float,\n",
        "        on: Literal[\"step\", \"episode\"],\n",
        "    ) -> None:\n",
        "        super().__init__(initial=initial, on=on)\n",
        "        self.final = final\n",
        "        self.rate = rate\n",
        "\n",
        "    def _update(self) -> None:\n",
        "        self._value = max(self._value - self.rate, self.final)\n",
        "\n",
        "\n",
        "class ExponentialRateScheduler(ExplorationRateScheduler):\n",
        "    \"\"\"Exponential scheduler (step() multiplies value by `decay`, until final is reached).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        initial: float,\n",
        "        final: float,\n",
        "        decay: float,\n",
        "        on: Literal[\"step\", \"episode\"],\n",
        "    ) -> None:\n",
        "        super().__init__(initial=initial, on=on)\n",
        "        self.final = final\n",
        "        self.decay = decay\n",
        "\n",
        "    def _update(self) -> None:\n",
        "        self._value = max(self._value * self.decay, self.final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPds8uarDZUE"
      },
      "source": [
        "## Replay buffer\n",
        "\n",
        "The key trick that makes DQN feasible is the replay buffer. The idea is to store observed transitions, sample them randomly and perform updates based on them. This solution has many advantages, the most significant ones are:\n",
        "\n",
        "1.   *Data efficiency* – each transition (env step) can be used in many weight updates.\n",
        "2.   *Data decorrelation* – consecutive transitions are naturally highly correlated. Randomizing the samples reduces these correlations, thus reducing variance of the updates.\n",
        "\n",
        "Note that when learning by experience replay, it is necessary to learn off-policy (because our current parameters are different than those used to generate the sample), which motivates the choice of Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oI62-4PtDb2v"
      },
      "outputs": [],
      "source": [
        "class Transition(NamedTuple):\n",
        "    observation: np.ndarray  # shape OBS_SHAPE\n",
        "    action: int\n",
        "    next_observation: np.ndarray  # shape OBS_SHAPE\n",
        "    reward: float\n",
        "    is_terminal: bool  # (useful when using target_net for predicting qvalues)\n",
        "\n",
        "\n",
        "class TransitionBatch(NamedTuple):\n",
        "    observation: Tensor  # shape (batch_size, *OBS_SHAPE)\n",
        "    action: Tensor  # shape (batch_size,), dtype=torch.long\n",
        "    next_observation: Tensor  # shape (batch_size, *OBS_SHAPE)\n",
        "    reward: Tensor  # shape (batch_size,)\n",
        "    is_terminal: Tensor  # shape (batch_size,), dtype=torch.bool\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int) -> None:\n",
        "        \"\"\"Create new replay buffer of a given capacity.\"\"\"\n",
        "        self.capacity = capacity\n",
        "        self._storage = list[Transition]()\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def add(self, transition: Transition) -> None:\n",
        "        \"\"\"Add new experience to the buffer.\"\"\"\n",
        "        if len(self._storage) < self.capacity:\n",
        "            self._storage.append(transition)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = transition\n",
        "        self._next_idx = (self._next_idx + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size: int) -> list[Transition]:\n",
        "        \"\"\"Sample batch of experiences from the buffer.\"\"\"\n",
        "        return random.sample(self._storage, batch_size)\n",
        "\n",
        "    def sample_to_torch(self, batch_size: int, device: torch.device) -> TransitionBatch:\n",
        "        \"\"\"Sample batch of experiences from the buffer and convert it to torch tensors.\"\"\"\n",
        "        batch = self.sample(batch_size)\n",
        "        return TransitionBatch(\n",
        "            torch.tensor(np.array([t.observation for t in batch]), device=device),\n",
        "            torch.tensor([t.action for t in batch], dtype=torch.long, device=device),\n",
        "            torch.tensor(np.array([t.next_observation for t in batch]), device=device),\n",
        "            torch.tensor([t.reward for t in batch], dtype=torch.float32, device=device),\n",
        "            torch.tensor(\n",
        "                [t.is_terminal for t in batch], dtype=torch.bool, device=device\n",
        "            ),\n",
        "        )\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self._storage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCQIj0ycAim0"
      },
      "source": [
        "## MLP Network\n",
        "\n",
        "For fast iteration we will stick to numerical observations (original DQN paper works with graphical observations). We will use simple MLP to net approximate our estimates of Q-values for (action, states)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kBkWJjn-AglG"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_dims: Sequence[int] = (128, 128)) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        input_dim = reduce(operator.mul, OBS_SHAPE, 1)\n",
        "        dims = [input_dim, *hidden_dims, N_ACTIONS]\n",
        "\n",
        "        self.layers = nn.Sequential(nn.Flatten())\n",
        "\n",
        "        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:], strict=True)):\n",
        "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
        "            if i < len(dims) - 2:\n",
        "                self.layers.append(nn.ReLU())\n",
        "\n",
        "    def forward(self, state: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input: observation tensor of shape (batch_size, *OBS_SHAPE).\n",
        "        Output: Q-values tensor of shape (batch_size, N_ACTIONS).\n",
        "        \"\"\"\n",
        "        return self.layers(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKqQYX7HBBOw"
      },
      "source": [
        "## DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6C4yaDjB8FnV"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        eps_scheduler: ExplorationRateScheduler,\n",
        "        q_value_net: nn.Module,\n",
        "    ) -> None:\n",
        "        self.eps_scheduler = eps_scheduler\n",
        "        self.q_value_net = q_value_net\n",
        "\n",
        "    def act(self, obs: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Sample action using epsilon-greedy policy derived from q_value_net.\n",
        "\n",
        "        Args:\n",
        "            obs: shape OBS_SHAPE.\n",
        "\n",
        "        With probability epsilon=eps_scheduler.value(): return a random action.\n",
        "        Otherwise: return argmax_a Q_theta(obs, a)\n",
        "        \"\"\"\n",
        "        epsilon = self.eps_scheduler.value()\n",
        "        if torch.rand(1).item() < epsilon:\n",
        "            return int(torch.randint(low=0, high=N_ACTIONS, size=(1,)).item())\n",
        "        else:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "            obs_tensor = obs_tensor.view(1, *OBS_SHAPE)\n",
        "            with torch.no_grad():\n",
        "                return int(torch.argmax(self.q_value_net(obs_tensor)).item())\n",
        "\n",
        "    def save_checkpoint(self, path: Path) -> None:\n",
        "        \"\"\"Save q_value_net parameters to a checkpoint file.\"\"\"\n",
        "        torch.save(self.q_value_net.state_dict(), path)\n",
        "\n",
        "    def load_checkpoint(self, path: Path) -> None:\n",
        "        \"\"\"Load q_value_net parameters from a checkpoint file.\"\"\"\n",
        "        self.q_value_net.load_state_dict(torch.load(path, weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq01aStyMniA"
      },
      "source": [
        "## Playthrough (random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CkjlDn72MniA"
      },
      "outputs": [],
      "source": [
        "def display_episode_playthrough(agent: DQNAgent, env: gym.Env) -> None:\n",
        "    \"\"\"Run the agent to play one episode start-to-finish and show a rendering.\"\"\"\n",
        "    agent.q_value_net.eval()\n",
        "    observation, _info = env.reset()\n",
        "    terminated, truncated = False, False\n",
        "    total_reward = 0.0\n",
        "\n",
        "    image_widget = widgets.Image(format=\"jpeg\")\n",
        "    display.display(image_widget)\n",
        "\n",
        "    timestep = 0\n",
        "    while not (terminated or truncated):\n",
        "        timestep += 1\n",
        "        # Pick next action, simulate and observe next_state and reward\n",
        "        action = agent.act(observation)\n",
        "        observation, reward, terminated, truncated, _info = env.step(action)\n",
        "\n",
        "        frame = cv2.cvtColor(env.render(), cv2.COLOR_RGB2BGR)  # type: ignore\n",
        "        image_widget.value = cv2.imencode(\".jpeg\", frame)[1].tobytes()\n",
        "        time.sleep(0.01)\n",
        "\n",
        "        total_reward += float(reward)\n",
        "\n",
        "    print(f\"Episode length: {timestep}, total reward: {total_reward}\")\n",
        "    time.sleep(0.5)  # 0.5 s between episodes, so it's easier to watch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uqKwixzLMniA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e81b24af2e644cf98c85e3209e0318b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Image(value=b'', format='jpeg')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode length: 137, total reward: -286.6744656081555\n"
          ]
        }
      ],
      "source": [
        "display_episode_playthrough(\n",
        "    DQNAgent(ConstantRateScheduler(0.0), MLP(hidden_dims=[256, 256]).to(device)), env\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyYQhfREMniA"
      },
      "source": [
        "## DQN Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1K2SLGHuMniB"
      },
      "outputs": [],
      "source": [
        "class DQNTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        agent: DQNAgent,\n",
        "        optim: torch.optim.Optimizer,\n",
        "        env: gym.Env,\n",
        "        buffer_size: int = 10000,\n",
        "        gamma: float = 0.99,\n",
        "        batch_size: int = 128,\n",
        "        target_update_interval: int = 100,  # in steps\n",
        "        checkpoints_dir: Path = CHECKPOINTS_DIR,\n",
        "        checkpoint_save_interval: int = 5000,  # in steps\n",
        "        tensorboard_log_dir: Path = TENSORBOARD_LOG_DIR,\n",
        "    ) -> None:\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_interval = target_update_interval\n",
        "        self.optim = optim\n",
        "        self.checkpoints_dir = checkpoints_dir\n",
        "        self.checkpoint_save_interval = checkpoint_save_interval\n",
        "        self.tensorboard_log_dir = tensorboard_log_dir\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "        self.q_value_net = agent.q_value_net\n",
        "        self.target_net = deepcopy(self.q_value_net)\n",
        "\n",
        "        self._total_steps = 0\n",
        "\n",
        "    def train(self, n_episodes: int) -> None:\n",
        "        self.q_value_net.train()\n",
        "        self.target_net.load_state_dict(self.q_value_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self._total_steps = 0\n",
        "\n",
        "        rewards_history = list[float]()\n",
        "        run_id = len(list(self.tensorboard_log_dir.iterdir()))\n",
        "        with SummaryWriter(self.tensorboard_log_dir / f\"r{run_id}\") as writer:\n",
        "            progress_bar = tqdm(range(n_episodes), desc=\"Training\", unit=\"episode\")\n",
        "            for episode in progress_bar:\n",
        "                reward, length = self.train_episode(episode, writer)\n",
        "\n",
        "                # Logging to tqdm and Tensorboard\n",
        "                eps = self.agent.eps_scheduler.value()\n",
        "                rewards_history.append(reward)\n",
        "                reward_mean10 = np.mean(rewards_history[-10:])\n",
        "                progress_bar.set_postfix(\n",
        "                    {\n",
        "                        \"reward_mean10\": f\"{reward_mean10:5.1f}\",\n",
        "                        \"episode_length\": f\"{length}\",\n",
        "                        \"exploration_eps\": f\"{eps:.1g}\",\n",
        "                    }\n",
        "                )\n",
        "                writer.add_scalar(\"1_Reward/last1\", reward, episode)\n",
        "                writer.add_scalar(\"1_Reward/last10\", reward_mean10, episode)\n",
        "                writer.add_scalar(\"2_Other/episode_steps\", length, episode)\n",
        "                writer.add_scalar(\"2_Other/exploration_eps\", eps, episode)\n",
        "                self.agent.eps_scheduler.next_episode()\n",
        "\n",
        "    def train_episode(self, episode: int, writer: SummaryWriter) -> tuple[float, int]:\n",
        "        episode_reward, episode_steps = 0.0, 0\n",
        "\n",
        "        observation: np.ndarray\n",
        "        observation, _info = self.env.reset()\n",
        "        terminated, truncated = False, False\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            action = self.agent.act(observation)\n",
        "            next_observation: np.ndarray\n",
        "            next_observation, reward, terminated, truncated, _info = self.env.step(\n",
        "                action\n",
        "            )\n",
        "\n",
        "            # Store Transition in replay buffer. (\"state\", \"action\", \"next_state\", \"reward\", \"non_terminal_mask\")\n",
        "\n",
        "            is_terminal = terminated or truncated\n",
        "            self.replay_buffer.add(Transition(observation, action, next_observation, reward, is_terminal))\n",
        "\n",
        "            observation = next_observation\n",
        "\n",
        "            loss = self._update_q_value_net()\n",
        "\n",
        "            if (self._total_steps + 1) % self.target_update_interval == 0:\n",
        "                self._update_target_net()\n",
        "\n",
        "            if (self._total_steps + 1) % self.checkpoint_save_interval == 0:\n",
        "                ckpt_name = f\"step{self._total_steps + 1}_ep{episode}.ckpt\"\n",
        "                self.agent.save_checkpoint(self.checkpoints_dir / ckpt_name)\n",
        "\n",
        "            self._total_steps += 1\n",
        "            episode_steps += 1\n",
        "            episode_reward += float(reward)\n",
        "            self.agent.eps_scheduler.next_step()\n",
        "\n",
        "            if loss is not None:\n",
        "                writer.add_scalar(\"2_Other/loss\", loss, self._total_steps)\n",
        "\n",
        "        return episode_reward, episode_steps\n",
        "\n",
        "    def _update_target_net(self) -> None:\n",
        "        \"\"\"Copy q_value_net parameters into target_net.\"\"\"\n",
        "        self.target_net.load_state_dict(self.q_value_net.state_dict())\n",
        "\n",
        "        # Alternatively:\n",
        "        # for q, t in zip(self.q_value_net.parameters(), self.target_net.parameters(), strict=True):\n",
        "        #     q.data.copy_(t.data)\n",
        "\n",
        "    def _update_q_value_net(self) -> float | None:\n",
        "        \"\"\"Perform one round of q_value_net update.\n",
        "\n",
        "        Sample random minibatch of transitions (fi(s_t), a_t, r_t, fi(s_t+1)) from the replay buffer\n",
        "        and update q_value_net according to the DQN algorithm.\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        batch = self.replay_buffer.sample_to_torch(self.batch_size, device=device)\n",
        "\n",
        "        state_action_values = self.get_state_action_values(batch)\n",
        "        with torch.no_grad():\n",
        "            targets = self.get_targets(batch)\n",
        "\n",
        "        loss = F.mse_loss(state_action_values, targets)\n",
        "\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(self.q_value_net.parameters(), 100.0)\n",
        "        self.optim.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def get_targets(self, batch: TransitionBatch) -> Tensor:\n",
        "        \"\"\"Uses `target_net` and immediate rewards to calculate expected future rewards.\"\"\"\n",
        "        batch_size = batch.next_observation.shape[0]\n",
        "\n",
        "        q_value_of_next_obs, _argmax = torch.max(\n",
        "            self.target_net(batch.next_observation), dim=1\n",
        "        )\n",
        "        q_value_of_next_obs[batch.is_terminal] = 0.0\n",
        "\n",
        "        assert batch.reward.shape == (batch_size,)\n",
        "        assert q_value_of_next_obs.shape == (batch_size,)\n",
        "        # Expected future reward for terminal state is equal to immediate reward\n",
        "        # For non terminal states expected future reward:\n",
        "        #   immediate reward + discounted future expectation\n",
        "\n",
        "        targets = batch.reward + self.gamma * q_value_of_next_obs\n",
        "\n",
        "\n",
        "        assert targets.shape == (batch_size,)\n",
        "        return targets\n",
        "\n",
        "    def get_state_action_values(self, batch: TransitionBatch) -> Tensor:\n",
        "        \"\"\"\n",
        "        Use `q_value_net` to get estimates of future rewards for (obs, action).\n",
        "\n",
        "        Output shape: (batch_size,)\n",
        "        \"\"\"\n",
        "        batch_size = batch.observation.shape[0]\n",
        "        preds = self.q_value_net(batch.observation)  # shape (batch_size, n_actions).\n",
        "        # Select preds[i, action_index[i]] for i in batch.\n",
        "        return preds[torch.arange(batch_size), batch.action]\n",
        "        # Alternatively:\n",
        "        # return preds.gather(dim=1, index=batch.action.unsqueeze(dim=1)).squeeze(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtOafn2LMniC"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7bNUTHoMMniC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 5256), started 1:25:45 ago. (Use '!kill 5256' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-dd842eec6705279d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-dd842eec6705279d\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start tensorboard inside Google Colab\n",
        "# If you can't see anything, run this cell twice.\n",
        "%tensorboard --logdir $TENSORBOARD_LOG_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCDEETSwZP4I"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B0l9yUMniB"
      },
      "outputs": [],
      "source": [
        "n_episodes = 500\n",
        "estimated_n_steps = n_episodes * 100\n",
        "agent = DQNAgent(\n",
        "    q_value_net=MLP().to(device),\n",
        "    eps_scheduler=ExponentialRateScheduler(\n",
        "        initial=1.0, final=0.01, decay=0.995, on=\"episode\"\n",
        "    ),\n",
        "    # eps_scheduler=LinearRateScheduler(initial=1.0, final=0.01, decay=1 / estimated_n_steps, on=\"step\"),\n",
        ")\n",
        "trainer = DQNTrainer(\n",
        "    agent=agent,\n",
        "    optim=torch.optim.Adam(agent.q_value_net.parameters(), lr=0.0001),\n",
        "    env=env,\n",
        ")\n",
        "trainer.train(n_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf8emV6zMniB"
      },
      "source": [
        "## Playthrough (after training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "k9twCGQ59vnf"
      },
      "outputs": [],
      "source": [
        "# Load the latest checkpoint.\n",
        "agent = DQNAgent(q_value_net=MLP().to(device), eps_scheduler=ConstantRateScheduler(0.0))\n",
        "ckpt = sorted(CHECKPOINTS_DIR.iterdir(), key=lambda p: p.stat().st_mtime)[-1]\n",
        "agent.load_checkpoint(ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "F5mYymCEMniC"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2fdb1aa72f14260aa06c6a25cf27490",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Image(value=b'', format='jpeg')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode length: 310, total reward: 239.11061318981072\n"
          ]
        }
      ],
      "source": [
        "display_episode_playthrough(agent, env=env)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "jupyter2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
