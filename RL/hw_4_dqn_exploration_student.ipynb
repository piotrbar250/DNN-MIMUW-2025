{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration strategies for DQN algorithm\n",
        "\n",
        "In this assignment we are interested in exploration strategies that can be combined with Q-learning.\n",
        "Q-learning is an off-policy algorithm, which means that the data for the algorithm can be collected by a different policy (called behavioural policy) that the one the algorithm learns.\n",
        "\n",
        "Here we come across a classical trade-off in reinforcement learning, called exploration-exploitation trade-off. On the one hand, our behavioural policy should try out new state-action pairs to gain knowledge about their returns. On the other hand, when our estimate of returns is good enough, we would like to follow the state-action pairs with the highest estimated returns.\n",
        "\n",
        "We will be operating on DQN [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) algorithm and analyzing epsilon-greedy strategy, boltzmann and max-boltzmann strategy and combination of epsilon-greedy and boltzmann.\n",
        "We evaluate performance of DQN variants on the Lunar Lander environment.\n",
        "\n",
        "We provide an implementation of the DQN algorithm with random exploration strategy.\n",
        "Your goal is to implement the exploration variants by overriding appropriate methods of the provided class.\n"
      ],
      "metadata": {
        "id": "OCcuj7kCPowC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading\n",
        "\n",
        "To obtain the points for the assignment You need to provide the implementation of exploration techniques AND report with plots and conclusions.\n",
        "Measuring sensitivity means that You should at least examine one reasonably lower and one reasonably greater value of the considered hyperparameter (or the pair of hyperparameters).\n",
        "\n",
        "\n",
        "1. Implement epsilon-greedy strategy and investigate hyperparameter sensitivity (1 point).\n",
        "2. Implement epsilon-greedy strategy with epsilon annealing and investigate hyperparameter sensitivity (1 point).\n",
        "3. Implement boltzmann strategy and investigate hyperparameter sensitivity (1 point).\n",
        "4. Implement boltzmann strategy with temperature annealing and investigate hyperparameter sensitivity (1 point).\n",
        "5. Implement max-boltzmann strategy and investigate hyperparameter sensitivity (1 point).\n",
        "6. Implement max-boltzmann strategy with temperature annealing and investigate hyperparameter sensitivity (1 point).\n",
        "7. Implement combination of epsilon-greedy with epsilon annealing and boltzmann strategy and investigate hyperparameter sensitivity (1 point)\n",
        "8. (*) Bonus: propose another reasonable approach to combine epsilon-greedy with epsilon annealing strategy and boltzmann strategy and/or another reasonable strategy of temperature annealing for the boltzmann strategy (2 points).\n",
        "9. Compare methods, present plots and conclusions in a clear manner (3 points).\n",
        "\n",
        "You can obtain max 10 points, bonus points increase Your score, if You lose points in some other tasks."
      ],
      "metadata": {
        "id": "MdeRz_jnUBmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import necessary libraries."
      ],
      "metadata": {
        "id": "WugRYf0FYsxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "P3Ynynd4GGKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KKd6_LiXdh3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set hyperparameters of the training, set seeds for reproducibility and set weights initialization.\n",
        "Although for debugging it might be useful to operate on a smaller number of training_steps, seeds etc., in the final evaluation DO NOT CHANGE these parameters."
      ],
      "metadata": {
        "id": "kgtGd69HY7PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class parse_args:\n",
        "  def __init__(self):\n",
        "    self.n_seeds = 6\n",
        "    self.n_evaluate_episodes = 5\n",
        "    self.n_training_steps = 100000\n",
        "    self.buffer_size = 10000\n",
        "    self.init_steps = 10000\n",
        "    self.target_update_freq = 50\n",
        "    self.eval_freq = 1000\n",
        "    self.gym_id = \"LunarLander-v3\"\n",
        "    env = gym.make(self.gym_id)\n",
        "    self.state_dim = env.observation_space.shape[0]\n",
        "    self.batch_size = 128\n",
        "    self.hidden_dim = 128\n",
        "    self.action_dim = env.action_space.n\n",
        "    self.discount = 0.99\n",
        "    self.lr = 7e-4\n",
        "    self.cuda = True\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() and self.cuda else \"cpu\")\n",
        "\n",
        "args = parse_args()\n",
        "first_half_training_args = parse_args()\n",
        "first_half_training_args.n_training_steps = first_half_training_args.n_training_steps // 2\n",
        "second_half_training_args = parse_args()\n",
        "second_half_training_args.n_training_steps = second_half_training_args.n_training_steps // 2\n",
        "second_half_training_args.init_steps = 1"
      ],
      "metadata": {
        "id": "AbD6x1GJFgqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "def weight_init(model):\n",
        "  torch.nn.init.orthogonal_(model.weight.data)\n",
        "  model.bias.data.fill_(0.0)"
      ],
      "metadata": {
        "id": "wJ4x_6u9fQXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement the replay buffer.\n",
        "It has two methods: add one transition to the buffer and sample batch of transitions from the buffer."
      ],
      "metadata": {
        "id": "4nk9OsHCaPZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, args):\n",
        "    self.states = np.zeros((args.buffer_size, args.n_seeds, args.state_dim), dtype = np.float32)\n",
        "    self.actions = np.zeros((args.buffer_size, args.n_seeds), dtype = np.int64)\n",
        "    self.rewards = np.zeros((args.buffer_size, args.n_seeds), dtype = np.float32)\n",
        "    self.next_states = np.zeros((args.buffer_size, args.n_seeds, args.state_dim), dtype = np.float32)\n",
        "    self.terminals = np.zeros((args.buffer_size, args.n_seeds), dtype = np.int64)\n",
        "    self.idx = 0\n",
        "    self.current_size = 0\n",
        "    self.args = args\n",
        "\n",
        "  def add(self, state, action, reward, next_state, terminal):\n",
        "    if self.current_size < self.args.buffer_size:\n",
        "      self.current_size += 1\n",
        "    self.states[self.idx, :, :] = state\n",
        "    self.actions[self.idx, :] = action\n",
        "    self.rewards[self.idx, :] = reward\n",
        "    self.next_states[self.idx, :, :] = next_state\n",
        "    self.terminals[self.idx, :] = terminal\n",
        "    self.idx = (self.idx + 1) % self.args.buffer_size\n",
        "\n",
        "  def sample(self):\n",
        "    sample_idxs = np.random.permutation(self.current_size)[:self.args.batch_size]\n",
        "    states = torch.from_numpy(self.states[sample_idxs]).to(self.args.device)\n",
        "    actions = torch.from_numpy(self.actions[sample_idxs]).to(self.args.device)\n",
        "    rewards = torch.from_numpy(self.rewards[sample_idxs]).to(self.args.device)\n",
        "    next_states = torch.from_numpy(self.next_states[sample_idxs]).to(self.args.device)\n",
        "    terminals = torch.from_numpy(self.terminals[sample_idxs]).to(self.args.device)\n",
        "\n",
        "    return states, actions, rewards, next_states, terminals\n"
      ],
      "metadata": {
        "id": "8kswZi26I9_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement a simple Q network architecture with three layers and ReLU activations."
      ],
      "metadata": {
        "id": "WI1MmteualVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(torch.nn.Module):\n",
        "  def __init__(self, args):\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.layer_1 = torch.nn.Linear(args.state_dim, args.hidden_dim)\n",
        "    self.layer_2 = torch.nn.Linear(args.hidden_dim, args.hidden_dim)\n",
        "    self.layer_3 = torch.nn.Linear(args.hidden_dim, args.action_dim)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    self.layer_1.apply(weight_init)\n",
        "    self.layer_2.apply(weight_init)\n",
        "    self.layer_3.apply(weight_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer_1(x))\n",
        "    x = self.relu(self.layer_2(x))\n",
        "    x = self.layer_3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "jXq66twrc2Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we provide code for DQN with random exploration."
      ],
      "metadata": {
        "id": "NB2SjjLldglT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myEn1aKdaURQ"
      },
      "outputs": [],
      "source": [
        "TRAIN_SEED = 0\n",
        "EVAL_SEED = 1\n",
        "\n",
        "class DQN:\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    self.discount = self.args.discount\n",
        "    self.reset()\n",
        "    self.annealing = False\n",
        "\n",
        "  # Copying parameters of other DQN class by reference (for half epsion-greedy, half boltzmann task)\n",
        "  def copy_reference(self, other):\n",
        "    self.buffer = other.buffer\n",
        "    self.q_net = other.q_net\n",
        "    self.q_target = other.q_target\n",
        "    self.optimizer = other.optimizer\n",
        "\n",
        "  # Annealing of epsilon and/or temperature\n",
        "  def anneal(self, step):\n",
        "    pass\n",
        "\n",
        "  # Greedy action\n",
        "  def get_greedy_action(self, states):\n",
        "    with torch.no_grad():\n",
        "      action = torch.argmax(self.q_net(states), dim = -1).detach().cpu().numpy()\n",
        "      return action\n",
        "\n",
        "  # Exploration action choice\n",
        "  def explore(self, states):\n",
        "    # Random action choice\n",
        "    action = np.random.randint(self.args.action_dim, size = self.args.n_seeds)\n",
        "    return action\n",
        "\n",
        "  # Update of the main critic\n",
        "  def update(self):\n",
        "    states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "    with torch.no_grad():\n",
        "      q_next_states = torch.max(self.q_target(next_states), dim = -1)[0]\n",
        "    ones_tensor = torch.ones_like(terminals).to(self.args.device)\n",
        "    targets = rewards + (ones_tensor - terminals) * self.discount * q_next_states\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    q_values = self.q_net(states).gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "    loss = torch.mean((q_values - targets) ** 2)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  # Update of the targer critic\n",
        "  def update_target(self):\n",
        "    self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "  # Evaluation of the performance on test environments.\n",
        "  def evaluate(self):\n",
        "    eval_results = np.zeros(self.args.n_seeds)\n",
        "    with torch.no_grad():\n",
        "      eval_env = gym.make_vec(self.args.gym_id, num_envs = self.args.n_seeds, vectorization_mode=\"sync\")\n",
        "      eval_env.reset(seed = EVAL_SEED)\n",
        "      for _ in range(self.args.n_evaluate_episodes):\n",
        "        state, info = eval_env.reset()\n",
        "        episode_reward = np.zeros(self.args.n_seeds)\n",
        "        mask = np.ones(self.args.n_seeds)\n",
        "        while np.sum(mask) > 0:\n",
        "          action = self.get_greedy_action(torch.tensor(state).to(self.args.device))\n",
        "          next_state, reward, terminal, truncated, _ = eval_env.step(action)\n",
        "          episode_reward += mask * reward\n",
        "          state = next_state\n",
        "          mask *= (np.ones(self.args.n_seeds) - terminal) * (np.ones(self.args.n_seeds) - truncated)\n",
        "        eval_results += episode_reward / self.args.n_evaluate_episodes\n",
        "    return np.mean(eval_results), np.std(eval_results)\n",
        "\n",
        "\n",
        "  # Resetting the algorithm\n",
        "  def reset(self):\n",
        "    self.buffer = ReplayBuffer(self.args)\n",
        "    self.q_net = QNetwork(self.args).to(self.args.device) # main critic\n",
        "    self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr = self.args.lr, eps = 1e-5)\n",
        "    self.q_target = QNetwork(self.args).to(self.args.device) # target critic\n",
        "    self.update_target()\n",
        "\n",
        "  # Training loop\n",
        "  def train(self):\n",
        "    eval_results_means = np.array([])\n",
        "    eval_results_stds = np.array([])\n",
        "    train_env = gym.make_vec(self.args.gym_id, num_envs = self.args.n_seeds, vectorization_mode=\"sync\")\n",
        "    state, info = train_env.reset(seed = TRAIN_SEED)\n",
        "    mask = np.ones(self.args.n_seeds)\n",
        "    for step in range(self.args.n_training_steps):\n",
        "      action = self.explore(torch.tensor(state).unsqueeze(0).to(self.args.device))\n",
        "      if self.annealing:\n",
        "        self.anneal(step)\n",
        "      next_state, reward, terminal, truncated, _ = train_env.step(action)\n",
        "      self.buffer.add(state, action, reward, next_state, terminal)\n",
        "      state = next_state\n",
        "      if step % self.args.eval_freq == 0:\n",
        "          print(f\"Training step: {step}\")\n",
        "          eval_mean, eval_std = self.evaluate()\n",
        "          print(f\"Eval mean: {eval_mean}; eval_std: {eval_std}\")\n",
        "          eval_results_means = np.append(eval_results_means, eval_mean)\n",
        "          eval_results_stds = np.append(eval_results_stds, eval_std)\n",
        "      if step >= self.args.init_steps:\n",
        "        self.update()\n",
        "        if step % self.args.target_update_freq == 0:\n",
        "          self.update_target()\n",
        "      mask *= (np.ones(self.args.n_seeds) - terminal) * (np.ones(self.args.n_seeds) - truncated)\n",
        "      if np.sum(mask) == 0:\n",
        "        state, info = train_env.reset()\n",
        "        mask = np.ones(self.args.n_seeds)\n",
        "\n",
        "    return eval_results_means, eval_results_stds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement functions for plotting."
      ],
      "metadata": {
        "id": "2JDGKsz8chmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth(data, weigth = 0.9):\n",
        "  smooth_data = np.copy(data)\n",
        "  for index in range(1, len(data)):\n",
        "    smooth_data[index] = smooth_data[index - 1] * weigth + data[index] * (1.0 - weigth)\n",
        "\n",
        "  return smooth_data\n",
        "\n",
        "def plot_smooth(args, result_means, result_stds):\n",
        "  smooth_result_means = smooth(result_means)\n",
        "  smooth_result_stds = smooth(result_stds)\n",
        "  print(smooth_result_means)\n",
        "  print(smooth_result_stds)\n",
        "  xs = np.arange(len(result_means)) * args.eval_freq\n",
        "  print(xs)\n",
        "  plt.plot(xs, smooth_result_means, color = \"blue\")\n",
        "  plt.fill_between(xs, smooth_result_means - smooth_result_stds, smooth_result_means + smooth_result_stds, alpha = 0.2, label = \"smoothed_rewards\")\n",
        "  plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "def plot_smooth_many(args, result_means_list, result_stds_list, names_list, colours_list):\n",
        "  plt.figure(figsize=(12.8, 9.6))\n",
        "  for result_means, result_stds, name, colour in zip(result_means_list, result_stds_list, names_list, colours_list):\n",
        "    smooth_result_means = smooth(result_means)\n",
        "    smooth_result_stds = smooth(result_stds)\n",
        "    print(smooth_result_means)\n",
        "    print(smooth_result_stds)\n",
        "    xs = np.arange(len(result_means)) * args.eval_freq\n",
        "    print(xs)\n",
        "    plt.plot(xs, smooth_result_means, color = colour)\n",
        "    plt.fill_between(xs, smooth_result_means - smooth_result_stds, smooth_result_means + smooth_result_stds, alpha = 0.2, color = colour, label = f\"smoothed_rewards_{name}\")\n",
        "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "def plot_results(result_mean, result_std):\n",
        "  plot_smooth(args, result_mean, result_std)\n",
        "\n",
        "def plot_results_many(result_means_list, result_stds_list, name_list, colours_list):\n",
        "  plot_smooth_many(args, result_means_list, result_stds_list, name_list, colours_list)"
      ],
      "metadata": {
        "id": "P_zLGVqHpvQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we provide code for training across different random seeds."
      ],
      "metadata": {
        "id": "hoiLn1y1cshT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(dqn):\n",
        "  set_seed(TRAIN_SEED)\n",
        "  dqn.reset()\n",
        "  result_mean, result_std = dqn.train()\n",
        "  print(result_mean)\n",
        "  return result_mean, result_std\n"
      ],
      "metadata": {
        "id": "XLG6xzJ-o7oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN(args)\n",
        "result_means_dqn, result_stds_dqn = train_dqn(dqn)\n",
        "plot_results(result_means_dqn, result_stds_dqn)"
      ],
      "metadata": {
        "id": "R2PyyNFdq50-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the goal is to implement the epsilon-gredy strategy. With probability epsilon we choose uniformly a random action and with probability 1-epsilon we take the action with the highest Q-value according to the main critic."
      ],
      "metadata": {
        "id": "8BHzDjEUfYn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyDQN(DQN):\n",
        "  def __init__(self, args):\n",
        "    super(EpsilonGreedyDQN, self).__init__(args)\n",
        "    self.epsilon = 0.1 # investigate sensitivity\n",
        "\n",
        "  def explore(self, states):\n",
        "    action = None\n",
        "    # TODO\n",
        "    ####################################\n",
        "    ####################################\n",
        "    return action"
      ],
      "metadata": {
        "id": "v6lj71xe7c77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_greedy_dqn = EpsilonGreedyDQN(args)\n",
        "result_means_epsilon_greedy_dqn, result_stds_epsilon_greedy_dqn = train_dqn(epsilon_greedy_dqn)\n",
        "plot_results(result_means_epsilon_greedy_dqn, result_stds_epsilon_greedy_dqn)"
      ],
      "metadata": {
        "id": "JefbwjjR9AU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we add to the epsilon-greedy strategy epsilon annealing. We change linearly epsilon from 1.0 to the value final_epsilon during first anneal_steps steps and then it remains on the final_epsilon level.\n",
        "Such an approach aims to increase the exploration level at the beginning of the training, when the Q-value estimate is poor and thus choosing greedily according to Q is not improving the performance."
      ],
      "metadata": {
        "id": "1iKU0YllgL5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyWithAnnealingDQN(EpsilonGreedyDQN):\n",
        "  def __init__(self, args):\n",
        "    self.start_epsilon = 1.0\n",
        "    super(EpsilonGreedyWithAnnealingDQN, self).__init__(args)\n",
        "    self.epsilon = self.start_epsilon\n",
        "    self.final_epsilon = 0.1 # investigate sensitivity\n",
        "    self.annealing = True\n",
        "    self.anneal_steps = 30000\n",
        "\n",
        "  def anneal(self, step):\n",
        "    # TODO\n",
        "    ####################################\n",
        "    ####################################\n",
        "\n",
        "  def reset(self):\n",
        "    super(EpsilonGreedyWithAnnealingDQN, self).reset()\n",
        "    self.epsilon = self.start_epsilon"
      ],
      "metadata": {
        "id": "4VRBq0D-_Ug7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_greedy_with_annealing_dqn = EpsilonGreedyWithAnnealingDQN(args)\n",
        "result_means_epsilon_greedy_with_annealing_dqn, result_stds_epsilon_greedy_with_annealing_dqn = train_dqn(epsilon_greedy_with_annealing_dqn)\n",
        "plot_results(result_means_epsilon_greedy_with_annealing_dqn, result_stds_epsilon_greedy_with_annealing_dqn)"
      ],
      "metadata": {
        "id": "ZY1C0OjtA8fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative approach to the epsilon-greedy strategy is to use so-called boltzmann exploration strategy.\n",
        "The idea behind this approach is to perform softmax on the Q-values coming from the main critic and then sample from the obtained distribution.\n",
        "In this approach we use softmax with a temperature, i.e. before applying softmax, we scale all the Q-values by the temperature coefficient (in the literature we usually divide by the temperature, but this is equivallent to scaling by the inverse of the temperature). Large scaling values make the distribution close to the greedy choice, while low scaling values make the distribution close to the uniform one."
      ],
      "metadata": {
        "id": "2VJ4wifPiE55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BoltzmannDQN(DQN):\n",
        "  def __init__(self, args):\n",
        "    super(BoltzmannDQN, self).__init__(args)\n",
        "    self.temperature = 1.0 # investigate sensitivity\n",
        "\n",
        "  def explore(self, states):\n",
        "    action = None\n",
        "    with torch.no_grad():\n",
        "      # TODO\n",
        "      ####################################\n",
        "      ####################################\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "_0wB5TcYRGAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boltzmann_dqn = BoltzmannDQN(args)\n",
        "result_means_boltzmann_dqn, result_stds_boltzmann_dqn = train_dqn(boltzmann_dqn)\n",
        "plot_results(result_means_boltzmann_dqn, result_stds_boltzmann_dqn)"
      ],
      "metadata": {
        "id": "0wcy8QOyijxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the compromises between epsilon-greedy and boltzmann exploration strategy is so-calles max-boltzmann strategy. In this strategy with probability 1-epsilon we choose action greedily, but with probability epsilon we perform the boltzmann choice instead of the uniform random choice."
      ],
      "metadata": {
        "id": "oLyezLVijx3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxBoltzmannDQN(EpsilonGreedyWithAnnealingDQN):\n",
        "  def __init__(self, args):\n",
        "    super(MaxBoltzmannDQN, self).__init__(args)\n",
        "    self.temperature = 0.1 # investigate sensitivity\n",
        "\n",
        "  def explore(self, states):\n",
        "    action = None\n",
        "    with torch.no_grad():\n",
        "      # TODO\n",
        "      ####################################\n",
        "      ####################################\n",
        "    return action"
      ],
      "metadata": {
        "id": "5uaDREL-hIkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_boltzmann_dqn = MaxBoltzmannDQN(args)\n",
        "result_means_max_boltzmann_dqn, result_stds_max_boltzmann_dqn = train_dqn(max_boltzmann_dqn)\n",
        "plot_results(result_means_max_boltzmann_dqn, result_stds_max_boltzmann_dqn)"
      ],
      "metadata": {
        "id": "QjR01lGcw1jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to adjusting the value of epsilon in epsilon-greedy strategy, we can adjust the temperature in the max-boltzmann and boltzmann strategies: we start we the value start_temperature and linearly increase the value to the final_temperature during temperature_anneal_steps, then the temperature is on the constant level.\n"
      ],
      "metadata": {
        "id": "rq_pxa8KkbIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxBoltzmannWithTemperatureAnnealingDQN(MaxBoltzmannDQN):\n",
        "  def __init__(self, args):\n",
        "    self.start_temparature = 0.025 # investigate sensitivity\n",
        "    super(MaxBoltzmannWithTemperatureAnnealingDQN, self).__init__(args)\n",
        "    self.temperature = self.start_temparature\n",
        "    self.final_temperature = 0.3 # investigate sensitivity\n",
        "    self.temperature_anneal_steps = 30000\n",
        "    self.annealing = True\n",
        "\n",
        "  def anneal(self, step):\n",
        "    super(MaxBoltzmannWithTemperatureAnnealingDQN, self).anneal(step)\n",
        "    # TODO\n",
        "    ####################################\n",
        "    ####################################\n",
        "\n",
        "  def reset(self):\n",
        "    super(MaxBoltzmannWithTemperatureAnnealingDQN, self).reset()\n",
        "    self.temperature = self.start_temparature"
      ],
      "metadata": {
        "id": "3naKqfLVDwP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_boltzmann_temp_anneal_dqn = MaxBoltzmannWithTemperatureAnnealingDQN(args)\n",
        "result_means_max_boltzmann_temp_anneal_dqn, result_stds_max_boltzmann_temp_anneal_dqn = train_dqn(max_boltzmann_temp_anneal_dqn)\n",
        "plot_results(result_means_max_boltzmann_temp_anneal_dqn, result_stds_max_boltzmann_temp_anneal_dqn)"
      ],
      "metadata": {
        "id": "HidAoBNCFV6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoltzmannWithTemperatureAnnealingDQN(BoltzmannDQN):\n",
        "  def __init__(self, args):\n",
        "    self.start_temparature = 0.25 # investigate sensitivity\n",
        "    super(BoltzmannWithTemperatureAnnealingDQN, self).__init__(args)\n",
        "    self.temperature = self.start_temparature\n",
        "    self.final_temperature = 3.0 # investigate sensitivity\n",
        "    self.temperature_anneal_steps = 30000\n",
        "    self.annealing = True\n",
        "\n",
        "  def anneal(self, step):\n",
        "    # TODO\n",
        "    ####################################\n",
        "    ####################################\n",
        "\n",
        "  def reset(self):\n",
        "    super(BoltzmannWithTemperatureAnnealingDQN, self).reset()\n",
        "    self.temperature = self.start_temparature"
      ],
      "metadata": {
        "id": "ylINCOiLb09P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boltzmann_temp_anneal_dqn = BoltzmannWithTemperatureAnnealingDQN(args)\n",
        "result_means_boltzmann_temp_anneal_dqn, result_stds_boltzmann_temp_anneal_dqn = train_dqn(boltzmann_temp_anneal_dqn)\n",
        "plot_results(result_means_boltzmann_temp_anneal_dqn, result_stds_boltzmann_temp_anneal_dqn)"
      ],
      "metadata": {
        "id": "R--aoCSzjEjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last exploration idea we want to implement is a combintation of the epsilon-greedy strategy (with epsilon annealing) and the boltzmann strategy.\n",
        "We could think that at the beginning of the training the boltzmann strategy struggles because the Q-function (the main critic) is not yet well-trained. However, the more critic is trained, the more sense it makes to start using the boltzmann strategy. We would like to verif y this hypoothesis by using in the first half of the training epsilon-greedy strategy (with epsilon annealing) and in the second half of the training switch the exploration strategy to the boltzmann one."
      ],
      "metadata": {
        "id": "-jcSRBHXldKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_two_halfs_dqn(dqn_1, dqn_2):\n",
        "  set_seed(TRAIN_SEED)\n",
        "  # TODO\n",
        "  ####################################\n",
        "  ####################################\n",
        "\n",
        "  return result_mean, result_std\n"
      ],
      "metadata": {
        "id": "AjaSq-vC1PTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_greedy_with_annealing_half_dqn = EpsilonGreedyWithAnnealingDQN(first_half_training_args) # investigate sensitivity of epsilon\n",
        "epsilon_greedy_boltzmann_half_dqn = BoltzmannDQN(second_half_training_args) # investigate sensitivity of temperature\n",
        "result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn, result_stds_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn = train_two_halfs_dqn(epsilon_greedy_with_annealing_half_dqn, epsilon_greedy_boltzmann_half_dqn)\n",
        "plot_results(result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn, result_stds_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn)"
      ],
      "metadata": {
        "id": "YwVivxWM1pXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we plot the results of all exploration methods on one plot. However, for drawing conclusions, it might be reasonable to plot some subsets of methods together, for example to compare variants with and without annealing, max-boltzmann with boltzmann, epsilon-greedy, boltzmann and half-epsilon-greedy, half-boltzmann."
      ],
      "metadata": {
        "id": "EM7aIGQuQ8Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_means_list = [result_means_dqn, result_means_epsilon_greedy_dqn, result_means_epsilon_greedy_with_annealing_dqn,\n",
        "                result_means_boltzmann_dqn, result_means_boltzmann_temp_anneal_dqn,\n",
        "                result_means_max_boltzmann_dqn, result_means_max_boltzmann_temp_anneal_dqn,\n",
        "                result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn]\n",
        "result_stds_list = [result_stds_dqn, result_stds_epsilon_greedy_dqn, result_stds_epsilon_greedy_with_annealing_dqn,\n",
        "                result_stds_boltzmann_dqn, result_stds_boltzmann_temp_anneal_dqn,\n",
        "                result_stds_max_boltzmann_dqn, result_stds_max_boltzmann_temp_anneal_dqn,\n",
        "                result_stds_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn]\n",
        "names_list = [\"random\", \"epsilon-greedy\", \"epsilon-greedy-with-annealing\",\n",
        "             \"boltzmann\", \"boltzmann-with-annealing\",\n",
        "             \"max-boltzmann\", \"max-boltzmann-with-annealing\",\n",
        "             \"half-epsilon-greedy-with-annealing_half-boltzmann\"]\n",
        "colours_list = [\"red\", \"green\", \"blue\",\n",
        "           \"yellow\", \"magenta\",\n",
        "           \"cyan\", \"black\",\n",
        "           \"orange\"]\n",
        "\n",
        "plot_results_many(result_means_list, result_stds_list, names_list, colours_list)"
      ],
      "metadata": {
        "id": "yDgWDm0IM4OS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}