{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84VetyCaGLyR"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you will implement some of the techniques discussed in the lecture.\n",
        "\n",
        "Below you are given a solution to the previous scenario. It has two serious drawbacks:\n",
        " * The output predictions do not sum up to one (i.e. the output is not a probability distribution), even though the images always contain exactly one digit.\n",
        " * It uses MSE coupled with output sigmoid, which can lead to saturation and slow convergence.\n",
        "\n",
        "**Task 0.** Implement a numerically stable version of softmax.\n",
        "\n",
        "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block, rather than compute the gradient over the softmax values.\n",
        "\n",
        "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
        "\n",
        "**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n",
        "\n",
        "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n",
        "\n",
        "The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZypYewcXywA"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm pandas\n",
        "!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P22HqX9AbO1a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from numpy.typing import NDArray\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "FloatNDArray = NDArray[np.float64]\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9jGPaZhbO2B"
      },
      "outputs": [],
      "source": [
        "def load_mnist(\n",
        "    path: Path = Path(\"mnist.npz\")\n",
        ") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n",
        "\n",
        "    Returns tuple of:\n",
        "    - x_train: shape (N_train, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n",
        "    - x_test: shape (N_test, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n",
        "\n",
        "    More: https://en.wikipedia.org/wiki/MNIST_database\n",
        "    \"\"\"\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n",
        "        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n",
        "\n",
        "    H = W = 28\n",
        "    N_train = len(x_train)\n",
        "    N_test = len(x_test)\n",
        "    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n",
        "    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n",
        "\n",
        "    x_train = x_train.reshape(N_train, H * W) / 255.0\n",
        "    x_test = x_test.reshape(N_test, H * W) / 255.0\n",
        "\n",
        "    y_train = np.zeros((N_train, 10), dtype=np.float64)\n",
        "    y_train[np.arange(N_train), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((N_test, 10))\n",
        "    y_test[np.arange(N_test), _y_test] = 1\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3gAyqw4bO1p"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: FloatNDArray) -> FloatNDArray:\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVG90fCXywB"
      },
      "source": [
        "## Warm-Up\n",
        "Implement a numerically stable version of softmax.  \n",
        "\n",
        "In general, softmax is defined as  \n",
        "$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n",
        "However, taking $e^{1000000}$ can result in NaN.  \n",
        "Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n",
        "\n",
        "Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rutQhoaXywC"
      },
      "outputs": [],
      "source": [
        "def unstable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def stable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    ## TODO\n",
        "    ###{\n",
        "    ###}\n",
        "\n",
        "\n",
        "### TESTS ###\n",
        "def _test_one(x: FloatNDArray, y: FloatNDArray) -> None:\n",
        "    r = stable_softmax(x)\n",
        "    assert r.shape == y.shape, f\"Expected shape {y.shape}, got {r.shape=}\"\n",
        "    assert np.isclose(np.ones(x.shape[0]), r.sum(axis=-1), atol=1e-5, rtol=0).all()\n",
        "    assert np.isclose(y, r, atol=1e-5, rtol=0).all()\n",
        "\n",
        "def test_stable_softmax() -> None:\n",
        "    x1 = np.random.rand(100, 32).astype(np.float64)\n",
        "    _test_one(x1, unstable_softmax(x1))\n",
        "\n",
        "    x2 = np.ones((10, 10, 32), dtype=np.float64) * 1e6\n",
        "    _test_one(x2, np.ones_like(x2) / x2.shape[-1])\n",
        "\n",
        "    print(\"OK\")\n",
        "\n",
        "test_stable_softmax()\n",
        "### TESTS END ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModelResults\n"
      ],
      "metadata": {
        "id": "WWFnNWF8fWJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgEA2XRRbO2X"
      },
      "outputs": [],
      "source": [
        "class ModelResults:\n",
        "    \"\"\"Just a helper class for gathering results in a nice table. Feel free to ignore.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Map from model name to map from lr to list of test accuracies.\n",
        "        self.results = dict[str, dict[float, list[float]]]()\n",
        "\n",
        "    def clear(self, model_name: str | None = None) -> None:\n",
        "        \"\"\"Forget results for a given model (defaults to all models).\"\"\"\n",
        "        if model_name:\n",
        "            if model_name in self.results:\n",
        "                del self.results[model_name]\n",
        "        else:\n",
        "            self.results = {}\n",
        "\n",
        "    def add_result(self, model_name: str, learning_rate: float, accuracy: float) -> None:\n",
        "        if model_name not in self.results:\n",
        "            self.results[model_name] = {}\n",
        "        if learning_rate not in self.results[model_name]:\n",
        "            self.results[model_name][learning_rate] = []\n",
        "        self.results[model_name][learning_rate].append(accuracy)\n",
        "\n",
        "    def display_results(self) -> None:\n",
        "        data = list[dict[str, Any]]()\n",
        "        for model_name, model_results in self.results.items():\n",
        "            for lr, accuracies in model_results.items():\n",
        "                mean_accuracy = np.mean(accuracies)\n",
        "                accuracy_summary = f\"{mean_accuracy:2.1%} ± {np.std(accuracies) * 100:.1f} p.p.\"\n",
        "                data.append({\n",
        "                    \"model\": model_name,\n",
        "                    \"lr\": lr,\n",
        "                    \"mean_accuracy\": mean_accuracy,\n",
        "                    \"accuracy\": accuracy_summary\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(data).sort_values(\"mean_accuracy\", ascending=False)\n",
        "        del df[\"mean_accuracy\"]\n",
        "        display(df.style.format({\"lr\": \"{:.1g}\"}).hide())\n",
        "\n",
        "    def evaluate_model(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        model_constructor: Callable[[Sequence[int]], Any],\n",
        "        layers: Sequence[int] = (784, 30, 10),\n",
        "        learning_rates: Sequence[float] = (1.0, 10.0, 100.0),\n",
        "        n_trainings: int = 3,\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        # Automatic model name with parameters.\n",
        "        if kwargs:\n",
        "            if tuple(layers) != (784, 30, 10):\n",
        "                model_name += \"[\" + \",\".join(str(n) for n in layers) + \"]\"\n",
        "\n",
        "            model_name += \"(\"\n",
        "            for k, v in kwargs.items():\n",
        "                if isinstance(v, (float,  np.floating)):\n",
        "                    model_name += f\"{k}={v:.1g},\"\n",
        "                else:\n",
        "                    model_name += f\"{k}={v},\"\n",
        "            model_name = model_name[:-1]\n",
        "            model_name += \")\"\n",
        "\n",
        "        # Train for each learning rate, n_trainings times.\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Checking {n_trainings} random trainings with with lr = {lr}\")\n",
        "            for i in range(n_trainings):\n",
        "                network = model_constructor(layers, **kwargs)\n",
        "                accuracy = network.train(\n",
        "                    (x_train, y_train),\n",
        "                    epochs=10,\n",
        "                    mini_batch_size=100,\n",
        "                    learning_rate=lr,\n",
        "                    test_data=(x_test, y_test),\n",
        "                )\n",
        "                self.add_result(model_name, lr, float(accuracy))\n",
        "\n",
        "\n",
        "model_results = ModelResults()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "The solution to the previous lab: an MLP network with MSE loss on sigmoid outputs, trained with plain SGD (batched)."
      ],
      "metadata": {
        "id": "kJD-EMvq6H2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: H * W = 28 * 28 = 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        \"\"\"\n",
        "        self.sizes = list(sizes)\n",
        "\n",
        "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n",
        "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
        "\n",
        "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
        "        # Weights are indexed by target node first.\n",
        "        self.weights = [\n",
        "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
        "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
        "        ]\n",
        "\n",
        "        self.num_layers = len(self.weights)   # = len(sizes) - 1\n",
        "\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n",
        "\n",
        "        Returns last layer activations, shape (B, N^last), values 0..1.\n",
        "        \"\"\"\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i) + (N^i,)  ==  (B, N^i)\n",
        "            g = sigmoid(g @ w.T + b)\n",
        "        return g\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Gradient descent step.\n",
        "        self.weights = [\n",
        "            w - learning_rate * grad_w\n",
        "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - learning_rate * grad_b\n",
        "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        \"\"\"\n",
        "        Backpropagation for a mini-batch (vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input, shape (B, N^0)\n",
        "        - y: target label (one-hot encoded), shape (B, N^last)\n",
        "\n",
        "        Returns (grads_w, grads_b), where:\n",
        "        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
        "        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n",
        "        \"\"\"\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        ### Copy from previous labs ###\n",
        "\n",
        "\n",
        "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Gradient of loss (MSE) over output activations.\n",
        "\n",
        "        Args:\n",
        "        - a: output activations, shape (B, N^last).\n",
        "        - y: target values (one-hot encoded labels), shape (B, N^last).\n",
        "\n",
        "        Returns gradients, shape (B, N^last).\n",
        "        \"\"\"\n",
        "        assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
        "        B, N_last = a.shape\n",
        "        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n",
        "\n",
        "    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n",
        "        \"\"\"\n",
        "        Compute accuracy: the ratio of correct answers for test_data.\n",
        "\n",
        "        Args:\n",
        "        - x_test_data: shape (B, N^0).\n",
        "        - y_test_data: shape (B, N^last).\n",
        "        \"\"\"\n",
        "        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n",
        "        targets = np.argmax(y_test_data, axis=1)\n",
        "        return np.mean(predictions == targets)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
        "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
        "        epochs: int = 2,\n",
        "        mini_batch_size: int = 100,\n",
        "        learning_rate: float = 1.0\n",
        "    ) -> np.float64:\n",
        "        x_train, y_train = training_data\n",
        "        progress_bar = tqdm(range(epochs), desc=\"Epoch\")\n",
        "        for epoch in progress_bar:\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                i_begin = i * mini_batch_size\n",
        "                i_end = (i + 1) * mini_batch_size\n",
        "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
        "            if test_data:\n",
        "                x_test, y_test = test_data\n",
        "                accuracy = self.evaluate(x_test, y_test)\n",
        "                progress_bar.set_postfix_str(f\"Test accuracy: {accuracy * 100:.2f} %\")\n",
        "\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "            return self.evaluate(x_test, y_test)\n",
        "        else:\n",
        "            return np.float64(-1)\n",
        "\n",
        "model_results.evaluate_model(model_name=\"Baseline\", model_constructor=Network, n_trainings=3)\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "id": "tnBAMfMP6IRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZIY72SXywD"
      },
      "source": [
        "## Task 1: softmax & cross-entropy loss\n",
        "Use softmax instead of coordinate-wise sigmoid and use negative-log-loss instead of MSE. Test to see if this improves convergence.   \n",
        "\n",
        "Hints:\n",
        "* When implementing backprop it's easier to consider these two functions as a single block, skipping the computation of the gradient over the softmax values, and going directly to gradients over logits (last pre-activations).\n",
        "* Softmax is only used after the last layer; previous layers (and their grad computations) can be unchanged.\n",
        "* Remember to update the forward pass in both places.\n",
        "* Loss for a mini-batch is the mean of losses for each dataitem in it, by convention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77iGbiSDXywD"
      },
      "outputs": [],
      "source": [
        "class Task1(Network):\n",
        "    def __init__(self, sizes: Sequence[int]):\n",
        "        super().__init__(sizes=sizes)\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        ## TODO\n",
        "        ###{\n",
        "        ###}\n",
        "        return g\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Forward pass.\n",
        "        # Activations (including input) of shapes (B, N^0), (B, N^1), ..., (B, N^last).\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "\n",
        "        ## TODO\n",
        "        ###{\n",
        "\n",
        "        ###}\n",
        "\n",
        "        # Backward pass.\n",
        "        ## TODO\n",
        "        ###{\n",
        "        grads_w = []  # Shapes (N^last, N^{last-1}), ..., (N^1, N^0).\n",
        "        grads_b = []  # Shapes (N^last,), ..., (N^1,).\n",
        "\n",
        "        grads_w.reverse()  # Now shapes (N^1, N^0), ..., (N^last, N^{last-1}).\n",
        "        grads_b.reverse()  # Now shapes (N^1,) ..., (N^last,).\n",
        "        ###}\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "\n",
        "model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3)\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvIk4RxTXywD"
      },
      "source": [
        "## Task 2: L2-regularization and momentum\n",
        "Implement L2-regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n",
        "A few notes:\n",
        "* do not regularize the biases\n",
        "* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3-03midXywD"
      },
      "outputs": [],
      "source": [
        "class Task2(Network):\n",
        "    def __init__(\n",
        "        self, sizes: Sequence[int], l2_factor: float = 1e-5, momentum: float = 0.2\n",
        "    ):\n",
        "        super().__init__(sizes=sizes)\n",
        "        ## TODO\n",
        "        ####{\n",
        "        ###}\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        ## TODO\n",
        "        ###{\n",
        "        ###}\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"L2&Momentum\",\n",
        "    model_constructor=Task2,\n",
        "    learning_rates=[10.0],\n",
        "    n_trainings=1,\n",
        "    l2_factor=1e-5,\n",
        "    momentum=0.2\n",
        ")\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1And2(Task2, Task1):\n",
        "    # A somewhat hacky but short way to mix Task1 and Task2.\n",
        "    # You could also just replace the superclass of Task2 to be Task1.\n",
        "    pass\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"Softmax&L2&Momentum\",\n",
        "    model_constructor=Task1And2,\n",
        "    learning_rates=[2.0],\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1\n",
        ")\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "id": "nnBGG1xo0F34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nLauKUXywE"
      },
      "source": [
        "## Task 3 (optional)\n",
        "Implement more variations of SGD:\n",
        "* AdamW (probably the most popular choice) or Adagrad,\n",
        "* dropout\n",
        "* some simple data augmentations (e.g. tiny rotations/shifts etc.).\n",
        "\n",
        "Again, test to see how these changes improve accuracy/convergence.  \n",
        "\n",
        "Quick reminders:\n",
        "* for AdamW, check the official [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)'s pseudocode or the original paper: [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).\n",
        "* for AdaGrad, check the Appendix of this notebook.\n",
        "* for dropout: during training only, zero-out each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X3hRIizXywE"
      },
      "outputs": [],
      "source": [
        "# Place for remaining parts of task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HCbwiW2XywE"
      },
      "source": [
        "## Task 4\n",
        "Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence and what learning rates work.\n",
        "\n",
        "As a start, you can try this slightly larger architecture: [784,100,30,10]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92OPX1uCXywE"
      },
      "outputs": [],
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xl3A1WSXywE"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRX8ith-XywF"
      },
      "source": [
        "## Adagrad (simplified version)\n",
        "\n",
        "Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n",
        "For parameter $p_i$ we maintain a variable $G_i$ (can be set to $0$ initially).\n",
        "Let $\\mathcal{L}$ be our loss without L2.   \n",
        "We update $G_i$ and $p_i$ each training step as follows:  \n",
        "$$\n",
        "G_i = G_i +  \\left(\\frac{\\partial \\mathcal{L}}{\\partial p_i}\\right)^2\\\\\n",
        "p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\mathcal{L}}{\\partial p_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfBEYR4rYXmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}